{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "5a1cc00e8c544287b396336742289065": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_37ed3edef84b423395bf5d5e8dcd2235",
       "IPY_MODEL_4b24ac715e1e4dd988bc4dd918dc942c",
       "IPY_MODEL_2df53804547a43ccb83f060fe576036b"
      ],
      "layout": "IPY_MODEL_b41787b13cfa4662b72bf1a6e026bca8"
     }
    },
    "37ed3edef84b423395bf5d5e8dcd2235": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c71171d7c4a0448bb9a86f9467b30151",
      "placeholder": "​",
      "style": "IPY_MODEL_4d8ce1ba0a894cec8f1ddeb4add678a9",
      "value": "Loading weights: 100%"
     }
    },
    "4b24ac715e1e4dd988bc4dd918dc942c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b9880f49897e43ff8acbe00cea7d5ea9",
      "max": 103,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_5e737c65ead74989a043c896f26c03ce",
      "value": 103
     }
    },
    "2df53804547a43ccb83f060fe576036b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_edbe3a3561784bdc8d9fc79577c6d71f",
      "placeholder": "​",
      "style": "IPY_MODEL_ebe936f722264a0385341c412353dcac",
      "value": " 103/103 [00:00&lt;00:00, 424.83it/s, Materializing param=pooler.dense.weight]"
     }
    },
    "b41787b13cfa4662b72bf1a6e026bca8": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c71171d7c4a0448bb9a86f9467b30151": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4d8ce1ba0a894cec8f1ddeb4add678a9": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b9880f49897e43ff8acbe00cea7d5ea9": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5e737c65ead74989a043c896f26c03ce": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "edbe3a3561784bdc8d9fc79577c6d71f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ebe936f722264a0385341c412353dcac": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "source": "# ============================================================\n# Cell 0: Install all dependencies (run once, then restart runtime)\n# ============================================================\n!pip -q install --no-cache-dir \\\n  \"numpy<2.0\" \"pandas==2.2.2\" \\\n  pymupdf==1.24.9 tqdm==4.66.5 \\\n  sentence-transformers==3.0.1 faiss-cpu==1.8.0 \\\n  langchain==0.2.16 langchain-community==0.2.16 langchain-text-splitters==0.2.2 \\\n  langchain-openai==0.1.23 \\\n  rank_bm25==0.2.2 \\\n  ragas==0.1.21 \\\n  openai>=1.40",
   "metadata": {
    "id": "06RYfOn1fkpE"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WFLd84dZLmnJ",
    "outputId": "7e46ba8a-326e-41a6-8fcd-9cfaca051812"
   },
   "execution_count": 17,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "BASE_DIR = \"/content/drive/MyDrive/Kerala_DIRs\"   # <-- this matches your screenshot\n",
    "PDF_DIR = os.path.join(BASE_DIR, \"01_canonical_pdfs\")\n",
    "MANIFEST_PATH = os.path.join(BASE_DIR, \"10_manifests\", \"manifest_final.csv\")\n",
    "\n",
    "assert os.path.isdir(PDF_DIR), f\"Missing PDF folder: {PDF_DIR}\"\n",
    "assert os.path.isfile(MANIFEST_PATH), f\"Missing manifest: {MANIFEST_PATH}\"\n",
    "\n",
    "print(\"✅ Found PDF_DIR:\", PDF_DIR)\n",
    "print(\"✅ Found manifest:\", MANIFEST_PATH)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UcDy9LXyeQ4t",
    "outputId": "b7eaa242-330f-438a-fc7e-2f6a796da3ff"
   },
   "execution_count": 18,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "✅ Found PDF_DIR: /content/drive/MyDrive/Kerala_DIRs/01_canonical_pdfs\n",
      "✅ Found manifest: /content/drive/MyDrive/Kerala_DIRs/10_manifests/manifest_final.csv\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(MANIFEST_PATH)\n",
    "\n",
    "# Required columns\n",
    "required_cols = {\"doc_id\", \"canonical_filename\"}\n",
    "missing = required_cols - set(df.columns)\n",
    "assert not missing, f\"manifest_final.csv missing columns: {missing}\"\n",
    "\n",
    "# Build Drive paths using canonical_filename (ignore canonical_path because it's Windows)\n",
    "df[\"pdf_path\"] = df[\"canonical_filename\"].apply(lambda fn: os.path.join(PDF_DIR, str(fn)))\n",
    "df[\"pdf_exists\"] = df[\"pdf_path\"].apply(os.path.isfile)\n",
    "\n",
    "print(\"Rows in manifest:\", len(df))\n",
    "print(\"PDFs found:\", int(df[\"pdf_exists\"].sum()))\n",
    "print(\"PDFs missing:\", int((~df[\"pdf_exists\"]).sum()))\n",
    "\n",
    "if (~df[\"pdf_exists\"]).any():\n",
    "    display(df.loc[~df[\"pdf_exists\"], [\"doc_id\", \"canonical_filename\", \"pdf_path\"]].head(20))\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1yVozGQAeU-K",
    "outputId": "a6cde359-732f-4082-e54e-17c30d5fe07f"
   },
   "execution_count": 19,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Rows in manifest: 436\n",
      "PDFs found: 436\n",
      "PDFs missing: 0\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import fitz  # PyMuPDF\n",
    "from tqdm import tqdm\n",
    "\n",
    "def extract_pages(pdf_path: str):\n",
    "    pages = []\n",
    "    with fitz.open(pdf_path) as doc:\n",
    "        for i in range(len(doc)):\n",
    "            text = doc.load_page(i).get_text(\"text\") or \"\"\n",
    "            text = text.strip()\n",
    "            pages.append({\"page\": i+1, \"text\": text})\n",
    "    return pages\n",
    "\n",
    "# Smoke test: open one PDF\n",
    "sample = df[df[\"pdf_exists\"]].iloc[0]\n",
    "pages = extract_pages(sample[\"pdf_path\"])\n",
    "print(\"Sample doc_id:\", sample[\"doc_id\"])\n",
    "print(\"Pages:\", len(pages))\n",
    "print(\"First page chars:\", len(pages[0][\"text\"]))\n",
    "print(pages[0][\"text\"][:500])\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Yn2C8MN-ecQ8",
    "outputId": "3db229b3-81f4-4f5a-fb33-cff6301fe30a"
   },
   "execution_count": 20,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Sample doc_id: KCDI000001\n",
      "Pages: 12\n",
      "First page chars: 2236\n",
      "Vol.:(0123456789)\n",
      "Journal of Industrial and Business Economics (2020) 47:519–530\n",
      "https://doi.org/10.1007/s40812-020-00170-x\n",
      "1 3\n",
      "A critique of the Indian government’s response \n",
      "to the COVID-19 pandemic\n",
      "Jayati Ghosh1 \n",
      "Received: 30 May 2020 / Revised: 2 July 2020 / Accepted: 3 July 2020 / Published online: 11 July 2020 \n",
      "© Associazione Amici di Economia e Politica Industriale 2020\n",
      "Abstract\n",
      "The most destructive effects of Covid-19 in India have not been the result of the dis-\n",
      "ease, but the nature of \n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "def safe_get(row, col):\n",
    "    if col in row and pd.notna(row[col]):\n",
    "        return row[col]\n",
    "    return \"\"\n",
    "\n",
    "# Try to pick the most useful title for display:\n",
    "# prefer original_filename if present, else title\n",
    "def choose_display_title(row):\n",
    "    orig = safe_get(row, \"original_filename\")\n",
    "    title = safe_get(row, \"title\")\n",
    "    return orig if str(orig).strip() else title\n",
    "\n",
    "page_docs = []\n",
    "failed = []\n",
    "\n",
    "for _, row in tqdm(df[df[\"pdf_exists\"]].iterrows(), total=int(df[\"pdf_exists\"].sum())):\n",
    "    pdf_path = row[\"pdf_path\"]\n",
    "    try:\n",
    "        base_meta = {\n",
    "            \"doc_id\": safe_get(row, \"doc_id\"),\n",
    "            \"canonical_filename\": safe_get(row, \"canonical_filename\"),\n",
    "            \"display_title\": choose_display_title(row),\n",
    "            \"year\": safe_get(row, \"year\"),\n",
    "            \"doi\": safe_get(row, \"doi\"),\n",
    "            \"geo_scope\": safe_get(row, \"geo_scope\"),\n",
    "            \"metadata_confidence\": safe_get(row, \"metadata_confidence\"),\n",
    "            # Your sheet shows 2 metric columns (primary/secondary). Names can vary,\n",
    "            # so we check a few possible column names safely.\n",
    "            \"metric_bucket_primary\": safe_get(row, \"metric_bucket_primary\") or safe_get(row, \"metric_buck_primary\") or safe_get(row, \"metric_bucket\") or safe_get(row, \"metric_buc\"),\n",
    "            \"metric_bucket_secondary\": safe_get(row, \"metric_bucket_secondary\") or safe_get(row, \"metric_buck_secondary\") or \"\",\n",
    "            \"sha256_hash\": safe_get(row, \"sha256_hash\"),\n",
    "            \"file_size_bytes\": safe_get(row, \"file_size_bytes\"),\n",
    "            \"source_folder\": safe_get(row, \"source_folder\"),\n",
    "        }\n",
    "\n",
    "        pages = extract_pages(pdf_path)\n",
    "        for p in pages:\n",
    "            if not p[\"text\"]:\n",
    "                continue\n",
    "            meta = dict(base_meta)\n",
    "            meta[\"page\"] = p[\"page\"]\n",
    "            meta[\"pdf_path\"] = pdf_path  # traceability\n",
    "            page_docs.append(Document(page_content=p[\"text\"], metadata=meta))\n",
    "\n",
    "    except Exception as e:\n",
    "        failed.append((safe_get(row, \"doc_id\"), pdf_path, str(e)))\n",
    "\n",
    "print(\"✅ Page-docs created:\", len(page_docs))\n",
    "print(\"❌ Failed PDFs:\", len(failed))\n",
    "failed[:5]\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y_KwSTXgekh7",
    "outputId": "37e16bf9-1896-4abd-e14f-f035106cabed"
   },
   "execution_count": 21,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 436/436 [00:59<00:00,  7.35it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "✅ Page-docs created: 6912\n",
      "❌ Failed PDFs: 2\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('KCDI000387',\n",
       "  '/content/drive/MyDrive/Kerala_DIRs/01_canonical_pdfs/kcdi000387_.pdf',\n",
       "  \"Cannot open empty file: filename='/content/drive/MyDrive/Kerala_DIRs/01_canonical_pdfs/kcdi000387_.pdf'.\"),\n",
       " ('KCDI000424',\n",
       "  '/content/drive/MyDrive/Kerala_DIRs/01_canonical_pdfs/kcdi000424_.pdf',\n",
       "  \"Failed to open file '/content/drive/MyDrive/Kerala_DIRs/01_canonical_pdfs/kcdi000424_.pdf'.\")]"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# STEP 1: Improved Chunking (larger chunks for academic papers)\n# ============================================================\n# Academic papers have dense paragraphs — 1500 chars with 300 overlap\n# keeps more context per chunk while still being under token limits.\n\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\n\nsplitter = RecursiveCharacterTextSplitter(\n    chunk_size=1500,\n    chunk_overlap=300,\n    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]  # added \". \" to prefer sentence boundaries\n)\n\nchunk_docs = splitter.split_documents(page_docs)\n\n# Store full page text as \"parent_text\" in metadata for parent-child retrieval later\npage_lookup = {}\nfor doc in page_docs:\n    key = (doc.metadata[\"doc_id\"], doc.metadata[\"page\"])\n    page_lookup[key] = doc.page_content\n\nfor chunk in chunk_docs:\n    key = (chunk.metadata[\"doc_id\"], chunk.metadata[\"page\"])\n    chunk.metadata[\"parent_text\"] = page_lookup.get(key, \"\")\n\nprint(f\"Chunks created: {len(chunk_docs)}\")\nprint(f\"Avg chunk length: {sum(len(c.page_content) for c in chunk_docs) / len(chunk_docs):.0f} chars\")\nprint(f\"Parent pages indexed: {len(page_lookup)}\")",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K5AcAUiCiNC3",
    "outputId": "47b84186-7d6e-44fa-bdaa-fca16b4b7e7c"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# STEP 2: Upgraded Embedding Model + FAISS Index\n# ============================================================\n# bge-base-en-v1.5 is a much stronger retrieval model than MiniLM.\n# It uses 768-dim embeddings and was trained specifically for retrieval.\n# On Colab GPU this takes ~15-20 min for ~20k chunks.\n\nfrom langchain_community.embeddings import HuggingFaceEmbeddings\nfrom langchain_community.vectorstores import FAISS\n\nEMBEDDING_MODEL = \"BAAI/bge-base-en-v1.5\"\n\nembeddings = HuggingFaceEmbeddings(\n    model_name=EMBEDDING_MODEL,\n    model_kwargs={\"device\": \"cuda\"},    # use GPU if available\n    encode_kwargs={\"normalize_embeddings\": True}  # bge models need L2 normalization\n)\n\n# Build the FAISS index — this is the slow step (~15-20 min on GPU)\nprint(f\"Embedding {len(chunk_docs)} chunks with {EMBEDDING_MODEL}...\")\nvectorstore = FAISS.from_documents(chunk_docs, embeddings)\n\n# Save to Drive so you don't have to re-embed every session\nINDEX_DIR = os.path.join(BASE_DIR, \"faiss_index_bge_base\")\nvectorstore.save_local(INDEX_DIR)\nprint(f\"FAISS index saved to: {INDEX_DIR}\")",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 188,
     "referenced_widgets": [
      "5a1cc00e8c544287b396336742289065",
      "37ed3edef84b423395bf5d5e8dcd2235",
      "4b24ac715e1e4dd988bc4dd918dc942c",
      "2df53804547a43ccb83f060fe576036b",
      "b41787b13cfa4662b72bf1a6e026bca8",
      "c71171d7c4a0448bb9a86f9467b30151",
      "4d8ce1ba0a894cec8f1ddeb4add678a9",
      "b9880f49897e43ff8acbe00cea7d5ea9",
      "5e737c65ead74989a043c896f26c03ce",
      "edbe3a3561784bdc8d9fc79577c6d71f",
      "ebe936f722264a0385341c412353dcac"
     ]
    },
    "id": "UCvLxCV7emqF",
    "outputId": "3722172e-5323-46b3-c7b0-3f70cc0fc3cd"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# STEP 2b: (OPTIONAL) Load saved index instead of re-embedding\n# ============================================================\n# After first run, you can skip the embedding step and load from Drive:\n# Uncomment the lines below and SKIP cell 7 on subsequent runs.\n\n# from langchain_community.embeddings import HuggingFaceEmbeddings\n# from langchain_community.vectorstores import FAISS\n#\n# EMBEDDING_MODEL = \"BAAI/bge-base-en-v1.5\"\n# embeddings = HuggingFaceEmbeddings(\n#     model_name=EMBEDDING_MODEL,\n#     model_kwargs={\"device\": \"cuda\"},\n#     encode_kwargs={\"normalize_embeddings\": True}\n# )\n# INDEX_DIR = os.path.join(BASE_DIR, \"faiss_index_bge_base\")\n# vectorstore = FAISS.load_local(INDEX_DIR, embeddings, allow_dangerous_deserialization=True)\n# print(f\"Loaded FAISS index from: {INDEX_DIR}\")",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tBvVApKweo4E",
    "outputId": "7a68a411-a5c4-4574-cbe0-467b28a0bf10"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# STEP 3: BM25 Retriever + Hybrid Ensemble\n# ============================================================\n# BM25 catches exact keyword matches (acronyms like MGNREGA, Kudumbashree,\n# LSG, CDS) that dense embeddings often miss. EnsembleRetriever combines both.\n\nfrom langchain.retrievers import EnsembleRetriever\nfrom langchain_community.retrievers import BM25Retriever\n\n# Build BM25 index over the same chunks\nprint(\"Building BM25 index...\")\nbm25_retriever = BM25Retriever.from_documents(chunk_docs, k=25)\n\n# Dense retriever from FAISS\ndense_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 25})\n\n# Combine: 60% dense + 40% BM25\n# (dense gets more weight because semantic matching is usually better,\n#  but BM25 rescues keyword-specific queries)\nensemble_retriever = EnsembleRetriever(\n    retrievers=[dense_retriever, bm25_retriever],\n    weights=[0.6, 0.4]\n)\n\nprint(\"Hybrid retriever ready (FAISS 60% + BM25 40%)\")",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_lDlf9UEerOS",
    "outputId": "b9d5e01f-6c17-49d1-b7fd-60ded34ab657"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# STEP 4: Cross-Encoder Reranker\n# ============================================================\n# The reranker takes the top-25 candidates from hybrid search and\n# re-scores them with a cross-encoder (much more accurate but slower).\n# We keep only the top-k after reranking.\n\nfrom sentence_transformers import CrossEncoder\nimport numpy as np\n\nRERANKER_MODEL = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\nreranker = CrossEncoder(RERANKER_MODEL, max_length=512)\n\ndef retrieve_and_rerank(query: str, top_k: int = 6):\n    \"\"\"Hybrid retrieve -> cross-encoder rerank -> return top_k.\"\"\"\n    # Step 1: Get candidates from hybrid retriever\n    candidates = ensemble_retriever.invoke(query)\n\n    if not candidates:\n        return []\n\n    # Step 2: Score each candidate with cross-encoder\n    pairs = [(query, doc.page_content) for doc in candidates]\n    scores = reranker.predict(pairs)\n\n    # Step 3: Sort by reranker score descending\n    scored = list(zip(candidates, scores))\n    scored.sort(key=lambda x: x[1], reverse=True)\n\n    # Return top_k with scores attached\n    results = []\n    for doc, score in scored[:top_k]:\n        doc.metadata[\"reranker_score\"] = float(score)\n        results.append(doc)\n\n    return results\n\n# Quick test\ntest_results = retrieve_and_rerank(\"What is the role of Kudumbashree in poverty reduction?\")\nprint(f\"Retrieved and reranked: {len(test_results)} chunks\")\nfor i, doc in enumerate(test_results, 1):\n    print(f\"\\n[{i}] score={doc.metadata['reranker_score']:.4f}  \"\n          f\"doc={doc.metadata['doc_id']}  p.{doc.metadata['page']}\")\n    print(f\"    {doc.page_content[:200]}...\")",
   "metadata": {
    "id": "4dLYOxZTrnvG"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# STEP 5: OpenAI LLM Generation with Citations\n# ============================================================\nimport os\nfrom getpass import getpass\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.prompts import ChatPromptTemplate\n\n# Set your OpenAI API key (will prompt you securely)\nif \"OPENAI_API_KEY\" not in os.environ:\n    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OpenAI API key: \")\n\n# Use gpt-4o-mini for cost efficiency; switch to \"gpt-4o\" for higher quality\nllm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.1)\n\nSYSTEM_PROMPT = \"\"\"You are an expert research assistant specializing in Kerala community development, \nlocal governance, decentralization, and Indian urban/rural development policy.\n\nYou answer questions using ONLY the provided source documents. Follow these rules strictly:\n\n1. Base every claim on the provided sources. Cite using [doc_id, p.X] format after each claim.\n2. If the sources do not contain enough information to answer, say so explicitly.\n3. Synthesize information across multiple sources when relevant.\n4. When sources disagree, note the disagreement and cite both sides.\n5. Keep answers focused, well-structured, and academic in tone.\n6. End with a \"Sources Used\" section listing all cited documents with their titles.\"\"\"\n\ndef format_context(docs):\n    \"\"\"Format retrieved documents into a context string for the LLM.\"\"\"\n    context_parts = []\n    for i, doc in enumerate(docs, 1):\n        md = doc.metadata\n        header = (f\"[Source {i}] doc_id={md.get('doc_id','?')} | \"\n                  f\"page={md.get('page','?')} | \"\n                  f\"title={str(md.get('display_title',''))[:100]} | \"\n                  f\"year={md.get('year','?')} | \"\n                  f\"geo={md.get('geo_scope','?')}\")\n        context_parts.append(f\"{header}\\n{doc.page_content}\")\n    return \"\\n\\n---\\n\\n\".join(context_parts)\n\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", SYSTEM_PROMPT),\n    (\"human\", \"\"\"Based on the following source documents, answer the question.\n\nSOURCES:\n{context}\n\nQUESTION: {question}\n\nProvide a thorough, well-cited answer:\"\"\")\n])\n\nchain = prompt | llm\n\nprint(\"LLM chain ready (gpt-4o-mini)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# STEP 6: Full RAG Pipeline (retrieve + rerank + generate)\n# ============================================================\n\ndef rag_query(question: str, top_k: int = 6, verbose: bool = True):\n    \"\"\"End-to-end RAG: hybrid retrieve -> rerank -> LLM answer with citations.\"\"\"\n\n    # 1. Retrieve and rerank\n    docs = retrieve_and_rerank(question, top_k=top_k)\n\n    if not docs:\n        return {\"answer\": \"No relevant documents found.\", \"sources\": [], \"docs\": []}\n\n    # 2. Format context\n    context = format_context(docs)\n\n    # 3. Generate answer\n    response = chain.invoke({\"context\": context, \"question\": question})\n    answer = response.content\n\n    # 4. Collect source info\n    sources = []\n    for doc in docs:\n        md = doc.metadata\n        sources.append({\n            \"doc_id\": md.get(\"doc_id\"),\n            \"page\": md.get(\"page\"),\n            \"title\": str(md.get(\"display_title\", \"\"))[:120],\n            \"year\": md.get(\"year\"),\n            \"reranker_score\": md.get(\"reranker_score\", 0),\n        })\n\n    if verbose:\n        print(\"=\" * 80)\n        print(f\"QUESTION: {question}\")\n        print(\"=\" * 80)\n        print(f\"\\n{answer}\\n\")\n        print(\"-\" * 40)\n        print(\"Retrieved Sources (ranked by relevance):\")\n        for i, s in enumerate(sources, 1):\n            print(f\"  [{i}] {s['doc_id']} p.{s['page']} \"\n                  f\"(score={s['reranker_score']:.3f}) — {s['title']}\")\n\n    return {\"answer\": answer, \"sources\": sources, \"docs\": docs}\n\n\n# --- Try it! ---\nresult = rag_query(\"What indicators are used to measure municipal service delivery performance in Kerala?\")\n",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# STEP 6b: More example queries to test the pipeline\n# ============================================================\n\nqueries = [\n    \"How does Kudumbashree contribute to women's empowerment and poverty alleviation?\",\n    \"What are the main challenges in solid waste management in Kerala municipalities?\",\n    \"How do studies measure road congestion or traffic conditions in Indian cities?\",\n    \"What role do gram panchayats play in local governance and decentralization?\",\n    \"What indicators measure electricity or water service reliability in Kerala?\",\n]\n\nfor q in queries:\n    rag_query(q, top_k=5)\n    print(\"\\n\\n\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# --- EVALUATION SECTION ---\n## Step 7: Build evaluation dataset and run RAGAS metrics\n\nThe cells below let you:\n1. Define ground-truth Q&A pairs with expected source doc_ids\n2. Run retrieval metrics (Hit Rate, MRR)\n3. Run generation metrics via RAGAS (faithfulness, relevancy, context precision/recall)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# STEP 7a: Evaluation Dataset (Ground Truth)\n# ============================================================\n# Fill in expected_doc_ids with doc_ids you KNOW should be retrieved.\n# Start with 10-15 and grow to 30-50 as you review results.\n# You can find relevant doc_ids by running rag_query and checking which\n# sources are returned — then verify if those are correct.\n\neval_dataset = [\n    {\n        \"question\": \"What indicators are used to measure municipal service delivery performance in Kerala?\",\n        \"expected_doc_ids\": [],   # <-- fill in after reviewing results, e.g. [\"KCDI000045\", \"KCDI000112\"]\n        \"ground_truth_answer\": \"\" # <-- optional: write a short reference answer\n    },\n    {\n        \"question\": \"How does Kudumbashree contribute to women's empowerment?\",\n        \"expected_doc_ids\": [],\n        \"ground_truth_answer\": \"\"\n    },\n    {\n        \"question\": \"What are the challenges in solid waste management in Kerala municipalities?\",\n        \"expected_doc_ids\": [],\n        \"ground_truth_answer\": \"\"\n    },\n    {\n        \"question\": \"What role do gram panchayats play in decentralized governance?\",\n        \"expected_doc_ids\": [],\n        \"ground_truth_answer\": \"\"\n    },\n    {\n        \"question\": \"How is literacy rate measured across Kerala districts?\",\n        \"expected_doc_ids\": [],\n        \"ground_truth_answer\": \"\"\n    },\n    {\n        \"question\": \"What are the main sources of revenue for local self-government institutions in Kerala?\",\n        \"expected_doc_ids\": [],\n        \"ground_truth_answer\": \"\"\n    },\n    {\n        \"question\": \"How do studies measure housing quality or housing conditions in Indian cities?\",\n        \"expected_doc_ids\": [],\n        \"ground_truth_answer\": \"\"\n    },\n    {\n        \"question\": \"What is the impact of MGNREGA on rural employment in Kerala?\",\n        \"expected_doc_ids\": [],\n        \"ground_truth_answer\": \"\"\n    },\n    {\n        \"question\": \"How is drinking water access measured at the ward or panchayat level?\",\n        \"expected_doc_ids\": [],\n        \"ground_truth_answer\": \"\"\n    },\n    {\n        \"question\": \"What are the health indicators used in community development indices?\",\n        \"expected_doc_ids\": [],\n        \"ground_truth_answer\": \"\"\n    },\n]\n\nprint(f\"Evaluation dataset: {len(eval_dataset)} questions\")\nprint(\"NOTE: Fill in expected_doc_ids after reviewing initial results!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# STEP 7b: Retrieval Metrics (Hit Rate, MRR)\n# ============================================================\n# These measure whether the correct documents appear in the top-k results.\n# Only works once you fill in expected_doc_ids above.\n\ndef evaluate_retrieval(eval_data, top_k=6):\n    \"\"\"Compute Hit Rate@k and MRR@k for the evaluation dataset.\"\"\"\n    hit_rates = []\n    mrrs = []\n\n    for item in eval_data:\n        expected = set(item[\"expected_doc_ids\"])\n        if not expected:\n            continue  # skip items without ground truth\n\n        docs = retrieve_and_rerank(item[\"question\"], top_k=top_k)\n        retrieved_ids = [d.metadata.get(\"doc_id\") for d in docs]\n\n        # Hit Rate: was ANY expected doc in the top-k?\n        hit = any(rid in expected for rid in retrieved_ids)\n        hit_rates.append(1.0 if hit else 0.0)\n\n        # MRR: reciprocal rank of the first expected doc found\n        rr = 0.0\n        for rank, rid in enumerate(retrieved_ids, 1):\n            if rid in expected:\n                rr = 1.0 / rank\n                break\n        mrrs.append(rr)\n\n    if not hit_rates:\n        print(\"No evaluation items have expected_doc_ids filled in yet!\")\n        return\n\n    print(f\"Evaluated {len(hit_rates)} questions at top_k={top_k}\")\n    print(f\"  Hit Rate@{top_k}: {np.mean(hit_rates):.3f}\")\n    print(f\"  MRR@{top_k}:      {np.mean(mrrs):.3f}\")\n\n    return {\"hit_rate\": np.mean(hit_rates), \"mrr\": np.mean(mrrs)}\n\n# Run it (will only produce results once expected_doc_ids are filled in)\nevaluate_retrieval(eval_dataset, top_k=6)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# STEP 7c: RAGAS Evaluation (Faithfulness, Relevancy, etc.)\n# ============================================================\n# RAGAS uses an LLM (OpenAI) to automatically evaluate RAG quality.\n# It measures:\n#   - faithfulness: is the answer grounded in the retrieved context?\n#   - answer_relevancy: does it actually answer the question?\n#   - context_precision: are the retrieved chunks relevant?\n#   - context_recall: did we retrieve all the needed context?\n#     (requires ground_truth_answer to be filled in)\n\nfrom ragas import evaluate\nfrom ragas.metrics import faithfulness, answer_relevancy, context_precision, context_recall\nfrom datasets import Dataset\n\ndef run_ragas_evaluation(eval_data, top_k=6):\n    \"\"\"Run RAGAS evaluation over the eval dataset.\"\"\"\n    questions = []\n    answers = []\n    contexts = []\n    ground_truths = []\n\n    for item in eval_data:\n        q = item[\"question\"]\n        result = rag_query(q, top_k=top_k, verbose=False)\n\n        questions.append(q)\n        answers.append(result[\"answer\"])\n        contexts.append([d.page_content for d in result[\"docs\"]])\n\n        gt = item.get(\"ground_truth_answer\", \"\")\n        ground_truths.append(gt if gt else result[\"answer\"])  # fallback to generated answer\n\n    # Build HuggingFace Dataset\n    ds = Dataset.from_dict({\n        \"question\": questions,\n        \"answer\": answers,\n        \"contexts\": contexts,\n        \"ground_truth\": ground_truths,\n    })\n\n    # Choose metrics (context_recall needs ground_truth to be meaningful)\n    metrics = [faithfulness, answer_relevancy, context_precision]\n    if any(item.get(\"ground_truth_answer\") for item in eval_data):\n        metrics.append(context_recall)\n\n    # Run evaluation\n    print(f\"Running RAGAS evaluation on {len(questions)} questions...\")\n    result = evaluate(ds, metrics=metrics)\n\n    print(\"\\n--- RAGAS Results ---\")\n    for metric_name, score in result.items():\n        print(f\"  {metric_name}: {score:.4f}\")\n\n    return result\n\n# Run it\nragas_results = run_ragas_evaluation(eval_dataset, top_k=6)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ]
}